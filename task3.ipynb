{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df29eda",
   "metadata": {
    "id": "1df29eda"
   },
   "source": [
    "Step 0. Unzip enron1.zip into the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32cfce",
   "metadata": {
    "id": "bf32cfce"
   },
   "source": [
    "Step 1. Traverse the dataset and create a Pandas dataframe. This is already done for you and should run without any errors. You should recognize Pandas from task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c5d195",
   "metadata": {
    "id": "20c5d195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 2140.2004-09-13.GP.spam.txt\n",
      "skipped 2042.2004-08-30.GP.spam.txt\n",
      "skipped 1414.2004-06-24.GP.spam.txt\n",
      "skipped 3304.2004-12-26.GP.spam.txt\n",
      "skipped 4201.2005-04-05.GP.spam.txt\n",
      "skipped 2698.2004-10-31.GP.spam.txt\n",
      "skipped 0754.2004-04-01.GP.spam.txt\n",
      "skipped 5105.2005-08-31.GP.spam.txt\n",
      "skipped 2526.2004-10-17.GP.spam.txt\n",
      "skipped 2649.2004-10-27.GP.spam.txt\n",
      "skipped 4350.2005-04-23.GP.spam.txt\n",
      "skipped 3364.2005-01-01.GP.spam.txt\n",
      "skipped 4566.2005-05-24.GP.spam.txt\n",
      "skipped 4142.2005-03-31.GP.spam.txt\n",
      "skipped 2248.2004-09-23.GP.spam.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_spam():\n",
    "    category = 'spam'\n",
    "    directory = './enron1/spam'\n",
    "    return read_category(category, directory)\n",
    "\n",
    "def read_ham():\n",
    "    category = 'ham'\n",
    "    directory = './enron1/ham'\n",
    "    return read_category(category, directory)\n",
    "\n",
    "def read_category(category, directory):\n",
    "    emails = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename), 'r') as fp:\n",
    "            try:\n",
    "                content = fp.read()\n",
    "                emails.append({'name': filename, 'content': content, 'category': category})\n",
    "            except:\n",
    "                print(f'skipped {filename}')\n",
    "    return emails\n",
    "\n",
    "ham = read_ham()\n",
    "spam = read_spam()\n",
    "\n",
    "df_ham = pd.DataFrame.from_records(ham)\n",
    "df_spam = pd.DataFrame.from_records(spam)\n",
    "df = pd.concat([df_ham, df_spam], ignore_index=True)\n",
    "\n",
    "\n",
    "# df = pd.DataFrame.from_records(ham)\n",
    "# df = df.append(pd.DataFrame.from_records(spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c23fd",
   "metadata": {
    "id": "1a1c23fd"
   },
   "source": [
    "Step 2. Data cleaning is a critical part of machine learning. You and I can recognize that 'Hello' and 'hello' are the same word but a machine does not know this a priori. Therefore, we can 'help' the machine by conducting such normalization steps for it. Write a function `preprocessor` that takes in a string and replaces all non alphabet characters with a space and then lowercases the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c447c901",
   "metadata": {
    "id": "c447c901"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(e):\n",
    "    cleaned = re.sub(r'[^a-zA-Z]', ' ', e)\n",
    "    cleaned = cleaned.lower()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32521d",
   "metadata": {
    "id": "ba32521d"
   },
   "source": [
    "Step 3. We will now train the machine learning model. All the functions that you will need are imported for you. The instructions explain how the work and hint at which functions to use. You will likely need to refer to the scikit learn documentation to see how exactly to invoke the functions. It will be handy to keep that tab open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1442d377",
   "metadata": {
    "id": "1442d377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "Confusion Matrix:\n",
      " [[695  12]\n",
      " [  5 320]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99       707\n",
      "        spam       0.96      0.98      0.97       325\n",
      "\n",
      "    accuracy                           0.98      1032\n",
      "   macro avg       0.98      0.98      0.98      1032\n",
      "weighted avg       0.98      0.98      0.98      1032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Step 1: Instantiate CountVectorizer\n",
    "vectorizer = CountVectorizer(preprocessor=preprocessor)\n",
    "\n",
    "# Step 2: Split the dataset into train and test sets\n",
    "X = df['content']\n",
    "y = df['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Transform the text data into feature vectors\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Step 4: Fit the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Step 5: Generate predictions on the test set\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n",
    "print('Classification Report:\\n', class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674d032",
   "metadata": {
    "id": "9674d032"
   },
   "source": [
    "Step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b7d78c9",
   "metadata": {
    "id": "6b7d78c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 41142\n",
      "Top 10 Positive Features (Spam):\n",
      "         feature  importance\n",
      "18020      http    0.948530\n",
      "28958    prices    0.880992\n",
      "25568        no    0.830795\n",
      "30849    remove    0.754530\n",
      "30850   removed    0.712527\n",
      "27036  paliourg    0.700734\n",
      "17320      here    0.689060\n",
      "24530      more    0.662717\n",
      "17251     hello    0.645492\n",
      "26137       off    0.641608\n",
      "Top 10 Negative Features (Ham):\n",
      "         feature  importance\n",
      "12789     enron   -1.520467\n",
      "36202    thanks   -1.501006\n",
      "2616   attached   -1.488542\n",
      "9646      daren   -1.448227\n",
      "11212       doc   -1.314941\n",
      "23792     meter   -1.251408\n",
      "9836       deal   -1.227553\n",
      "40382       xls   -1.199253\n",
      "25256      neon   -1.068413\n",
      "27978  pictures   -0.982330\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Display features created by CountVectorizer\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(f'Total features: {len(features)}')\n",
    "\n",
    "# Step 8: Access model coefficients\n",
    "coefficients = model.coef_[0]\n",
    "feature_importance = pd.DataFrame({'feature': features, 'importance': coefficients})\n",
    "\n",
    "# Step 9: Top 10 positive and negative features\n",
    "top_positive_features = feature_importance.nlargest(10, 'importance')\n",
    "top_negative_features = feature_importance.nsmallest(10, 'importance')\n",
    "\n",
    "print('Top 10 Positive Features (Spam):\\n', top_positive_features)\n",
    "print('Top 10 Negative Features (Ham):\\n', top_negative_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267e7ad",
   "metadata": {
    "id": "d267e7ad"
   },
   "source": [
    "Submission\n",
    "1. Upload the jupyter notebook to Forage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LI4u_ZUGToDQ",
   "metadata": {
    "id": "LI4u_ZUGToDQ"
   },
   "source": [
    "All Done!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
